<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<h2>Deep Reinforcement Learning Based Control in Continuous Action and State Spaces using Policy Gradients and Actor-Critic Networks</h2>
<p></p>
<p></p>
<h3>Experiments</h3>
<p></p>
<p><strong>Cart Pole with Discrete Action</strong></p>
<p>The cart-pole model(2) has four state variables</p>
<p><span class="math display">\[
x - \text{position of the cart on the track} \\
\theta - \text{angle of the pole with the vertical} \\
\bar{x} - \text{cart velocity} \\
\bar{\theta} - \text{rate of change of the angle}
\]</span></p>
<ul>
<li><p>Observation 1:* Comparing Fig 1 and Fig 2, it is clear that having a larger batch size helps in faster learning and better policies in terms of Average Returns.</p></li>
<li><p><em>Observation 2:</em>** **From Fig 3 it is clear that when we push up the probability of picking action at in state st in proportion to the ‘reward-to-go’ from that state-action pair—the sum of rewards achieved by starting in st, taking action at, and then acting according to the current policy forever after, rather than a trajectory centric policy gives much better policies.</p></li>
</ul>
<p>Also Advantage normalization gives slightly better policies.</p>
<ul>
<li><p>Observation 3: However having a baseline didn’t improve the policy as expected as shown in figure 3.</p></li>
</ul>
<figure>
<img src="Images/graph_small_batch.png" />
</figure>
<p></p>
<p>Figure 1</p>
<p></p>
<figure>
<img src="Images/graph_large_batch.png" />
</figure>
<p></p>
<p>Figure 2</p>
<figure>
<img src="Images/large_optimal.png" />
</figure>
<p>Figure 3</p>
<figure>
<img src="Images/with-without-critic.png" />
</figure>
<p>Figure 4</p>
<p></p>
<p><strong>Inverted Pendulum with Continuous Actions</strong></p>
<figure>
<img src="Images/pendulum_continuous.PNG" />
</figure>
<p>Figure 5</p>
<ul>
<li><p>Observation 1: The learning curves with two different network architectures is shown in Fig 6. Its clear from the graph that the 5 layered feed forward neural network learned better policies in lesser number of iterations.</p></li>
</ul>
<p></p>
<p>Command Line Code</p>
<pre><code>python train_pg.py InvertedPendulum-v2 --render -n 100 -b 5000 -e 5 -rtg --exp_name lb_continuous_5_layered_DeepNeuralNet -l 3 -lr 1e-2</code></pre>
<figure>
<img src="Images/inverted.png" />
</figure>
<p>Figure 6</p>
<p></p>
<p><strong>HALF CHEETAH</strong></p>
<p>Half-Cheetah(1), is a planar kinematic string of 9 links and 10 joints; the “paws” of Half-Cheetah will also be called joints. The angles of 4-th and 5-th joint are fixed, all the the others are controllable. Consequently, Half-Cheetah is a 6-degree-of-freedom walking robot.</p>
<p></p>
<figure>
<img src="Images/half-ch.PNG" />
</figure>
<p>Figure 7</p>
<figure>
<img src="Images/half-cheetah.png" />
</figure>
<p>Figure 8</p>
<p>Code Block</p>
<pre><code>python train_pg.py HalfCheetah-v2 -ep 150 --discount 0.9 -b 40000 -rtg -l 3 -s 32 -lr 4e-2 --exp_name half_cheetah</code></pre>
<p></p>
<p><em>Observation 1:</em> After a lot of hyper parameter tuning, the settings that gave an average return above 150 before 100 iterations is given in the code block below. It used an unusually high batch size and a 5 layered deep neural network without a critic. * *</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<h3>REFERENCES</h3>
<p></p>
<ol type="1">
<li><p>Learning to Control a 6-Degree-of-Freedom Walking Robot Paweł Wawrzynski,</p></li>
</ol>
<p>http://prac.elka.pw.edu.pl/~pwawrzyn/pub-s/0601_SLEAC.pdf</p>
<ol type="1">
<li><p>A. G. Barto, R. S. Sutton, and C. W. Anderson, “Neuronlike adaptive elements that can solve difficult learning control problems,” http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf</p></li>
</ol>
<p></p>
<p></p>
</body>
</html>
